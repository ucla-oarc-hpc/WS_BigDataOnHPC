{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f57ab94b-483a-4f6f-9372-994189ac9ac8",
   "metadata": {},
   "source": [
    "# Spark Basics\n",
    "\n",
    "This notebook will go over some simple PySpark tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2950b70e-3784-484d-836b-c2b13a72c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0e29db-e2aa-4f3f-a574-0b14327afafe",
   "metadata": {},
   "source": [
    "First, we will need to start the Spark Session\n",
    "\n",
    "We will name the Spark Session, MyPySpark\n",
    "\n",
    "We will also give 15 GB of memory to the Spark Driver Process.\n",
    "By default, Spark only gives the Driver a few GB's. \n",
    "\n",
    "The SparkContext is created with the `spark` object.\n",
    "We will limit the `sc` sparkContext object to give only ERROR messages.\n",
    "If not, you may see a lot of INFO messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ab60152-9d99-4b6e-9c20-eb591bc4af5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/24 23:11:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"MyPySpark\").config(\"spark.driver.memory\", \"15g\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c52782d-f0ee-482e-a0b1-09a5245e3ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://n6464.hoffman2.idre.ucla.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyPySpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2b8b6aa3ad90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24473f22-d5b4-409c-b117-3b0002c36fa3",
   "metadata": {},
   "source": [
    "We can see the details of the Spark object.\n",
    "The `local[*]` shows we will use all the CPU cores on this current compute node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af54de0-3ff0-4e65-9aa9-5bc33eab4487",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lets create a random number list with 200 numbers <br>Save the list to the variable `num_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b86e14b-8166-4560-ac67-90e35c63d566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52, 93, 15, 72, 61]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "num_data = []\n",
    "for i in range(0,200):\n",
    "    num_tmp = np.random.randint(1,100)\n",
    "    num_data.append(num_tmp)\n",
    "    \n",
    "#Print first 5 values from list\n",
    "num_data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4276462d-8620-4fa1-90e2-028ccbf28fd5",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "Now, Let's create a RDD object from the num_data list\n",
    "\n",
    "This RDD object will be saved as `num_map_rdd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea46ca24-cd9e-4011-9812-ad87e2d9b5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd = sc.parallelize(num_data)\n",
    "type(num_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dad8062-67f7-4b9e-94be-0109bfb62c52",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Use .count() to count the number of elements in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24eecfcd-dfd3-4af3-b828-fd2a17645c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8186c2-dad2-46b7-8786-7b4020a76fcb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Use .map() on the RDD (Spark) list. \n",
    "\n",
    "This will create a new RDD object with the map function. This \"lazy evaluation\" will NOT compute the results so will NOT have the final value. \n",
    "\n",
    "This RDD object will just contain the \"task\" of running x^2 to be computed when you need it. \n",
    "\n",
    "This .map() function would be very quick since it will not compute x^2 over the list \n",
    "\n",
    "This new RDD object is saved as `num_map_rdd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea109ffb-18dc-4736-9688-9f269b5b7a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_map_rdd = num_rdd.map(lambda x: x * x)\n",
    "num_map_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdb1504-670d-487c-ae8a-7bc6c6218d83",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now will will ask to print the first 5 values from x^2. \n",
    "\n",
    "Using .take(N) will return an array with the first N elements.\n",
    "\n",
    "Spark will now compute the x^2 values since we asked for the values.<br>This will be quicker since the RDD object is parallized over 36 cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca7f0ad1-8962-4514-8896-d15402b35fc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2704, 8649, 225, 5184, 3721]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_map_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9818ec83-5eda-4e2d-9538-b994e8347af2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lets use .filter() on the RDD object to return a new RDD object will only the numbers that pass the condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d02edc4-2874-4573-b265-214772643525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values in new filter RDD:  28\n",
      "First 5 values in the new RDD:  [3, 2, 2, 3, 7]\n"
     ]
    }
   ],
   "source": [
    "num_filter_rdd = num_rdd.filter(lambda x: x < 10)\n",
    "print(\"Number of values in new filter RDD: \", str(num_filter_rdd.count()))\n",
    "print(\"First 5 values in the new RDD: \", str(num_filter_rdd.take(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e4eca1-46dc-4f02-a35b-b56739f3b424",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Use .collect() on the RDD will return all the elements of the RDD object to a normal Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d489663-c48a-4982-9d34-c0267d96edc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[52, 93, 15, 72, 61]\n"
     ]
    }
   ],
   "source": [
    "num_filter_array = num_rdd.collect()\n",
    "print(type(num_filter_array))\n",
    "print(num_filter_array[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939f0a9c-01ff-4d0d-88f8-4d6f4ed4822f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Working with Spark and word text\n",
    "\n",
    "In this example, we will use data from Project Gutenberg\n",
    "\n",
    "\"The Hound of the Baskervilles, by Arthur Conan Doyle\n",
    "\n",
    "The download TXT file is at 3070.txt\n",
    "\n",
    "We will load this TXT file to a RDD object with `.textFile()`\n",
    "\n",
    "This will load the TXT by LINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79b09315-8851-46d2-bda8-15362672548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"3070.txt\"\n",
    "book_rdd = sc.textFile(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad81a82-d05e-4ec3-a088-6de8f05e6ab2",
   "metadata": {},
   "source": [
    "Print the first 10 lines of the Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "376d1197-df56-4ef0-a09f-0cf18d72c954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Project Gutenberg's The Hound of the Baskervilles, by Arthur Conan Doyle\",\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with',\n",
       " 'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
       " 're-use it under the terms of the Project Gutenberg License included',\n",
       " 'with this eBook or online at www.gutenberg.org',\n",
       " '',\n",
       " '',\n",
       " 'Title: The Hound of the Baskervilles',\n",
       " '']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_rdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d9a8ad-fc73-46d2-8379-0152b64170c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Count the number of rows in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c61a5291-fdb1-427d-89a4-7c2764776b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7729"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a03d71-88a3-49fd-925b-42131da8b6be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Lets split the lines into words\n",
    "\n",
    "We will split the book text into the individual words.\n",
    "\n",
    "The .flatMap() function can return multiple values for each element in the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51befe25-24b3-47a2-819d-1f9612cc7f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 62248\n",
      "Number of distinct words: 9885\n"
     ]
    }
   ],
   "source": [
    "words_rdd = book_rdd.flatMap(lambda x: x.split())\n",
    "num_words = words_rdd.count()\n",
    "print(\"Number of words: \" + str(num_words))\n",
    "\n",
    "num_distinct_words = words_rdd.distinct().count()\n",
    "print(\"Number of distinct words: \" + str(num_distinct_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b25933-75e7-4761-b887-8b3eb678a22a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We will split the book text into the individual words. \n",
    "\n",
    "We create a new \"pair\" RDD `key_value_rdd` with key/value pairs.\n",
    "\n",
    "First, this RDD will have (\"word\",1) for each element of the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52b9dfec-deb7-4d51-b216-6484f44ed871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Project', 1), (\"Gutenberg's\", 1), ('The', 1), ('Hound', 1), ('of', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_value_rdd = words_rdd.map(lambda x: (x,1))\n",
    "key_value_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933f4101-4031-4bc3-803f-362e8fda4cfb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We will use .reduceByKey() to combine all the same words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92a7444e-6a4e-4ab8-9c5a-d42d76f2f0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Project', 78), ('The', 217), ('Hound', 11), ('of', 1694), ('Arthur', 4)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_kv_rdd = key_value_rdd.reduceByKey(lambda x,y: x+y)\n",
    "word_kv_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b05746f-f4f6-425d-b289-7bb331075de7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "First, we will need to flip the key/value pair to show (N, word) where N is the number of occurrences of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0fef0d6-13e5-484e-b23f-a741a97715fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(78, 'Project'), (217, 'The'), (11, 'Hound'), (1694, 'of'), (4, 'Arthur')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flip_word_kv_rdd = word_kv_rdd.map(lambda x: (x[1],x[0]))\n",
    "flip_word_kv_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d56b95-90a2-400e-b67e-c6da9cc7066f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We will now use .sortByKey()\n",
    "\n",
    "This will sort the key/value pairs by decreasing occurrences of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c24001dc-9646-4382-b0f5-636e1819fc69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3230, 'the'), (1694, 'of'), (1562, 'and'), (1449, 'to'), (1279, 'a')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_results_rdd = flip_word_kv_rdd.sortByKey(False)\n",
    "word_results_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44342b64-3045-4324-9d88-d00983cbd961",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "There are many other functions within Spark that can do more linguistics type tasks with a RDD object. Like removing \"stop\" words and \"stemming\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypyspark",
   "language": "python",
   "name": "mypyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
