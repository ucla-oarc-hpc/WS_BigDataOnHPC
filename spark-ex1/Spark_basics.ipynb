{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84a4939d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Spark Basics\n",
    "\n",
    "This notebook will go over some simple PySpark tasks. \n",
    "\n",
    "We will start the Spark Session, which will be named \"MyPySpark\". \n",
    "\n",
    "Additionally, we will give 15 GB of memory to the Spark Driver Process. By default, Spark only gives the Driver a few GBs.\n",
    "\n",
    "The SparkContext is created with the spark object. We will limit the `sc` SparkContext object to give only ERROR messages. If not, you may see a lot of INFO messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d98c177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/28 20:56:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://n1046.hoffman2.idre.ucla.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyPySpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2ba5c21f92b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "import numpy as np\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MyPySpark\").config(\"spark.driver.memory\", \"15g\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18b8f02",
   "metadata": {},
   "source": [
    "# Create a Random Number List\n",
    "\n",
    "Next, we'll create a random number list with 200 numbers and save the list to the variable `num_data`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bcc5ac",
   "metadata": {},
   "source": [
    "We can see the details of the Spark object.\n",
    "The `local[*]` shows we will use all the CPU cores on this current compute node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e894a741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52, 93, 15, 72, 61]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_data = [np.random.randint(1,100) for _ in range(200)]\n",
    "\n",
    "# Print first 5 values from list\n",
    "num_data[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe34d722",
   "metadata": {},
   "source": [
    "# Create a Resilient Distributed Dataset (RDD)\n",
    "\n",
    "We'll create an RDD object from the `num_data` list. This RDD object will be saved as `num_rdd`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "127e3d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(pyspark.rdd.RDD, 200)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd = sc.parallelize(num_data)\n",
    "type(num_rdd), num_rdd.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf5c7ed",
   "metadata": {},
   "source": [
    "# Map Transformation\n",
    "\n",
    "We'll use the .map() method on the RDD (Spark) list. This will create a new RDD object with the map function. This \"lazy evaluation\" will NOT compute the results so will NOT have the final value. This RDD object will just contain the \"task\" of running x^2 to be computed when you need it. This .map() function would be very quick since it will not compute x^2 over the list. This new RDD object is saved as `num_map_rdd`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27740451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[2] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_map_rdd = num_rdd.map(lambda x: x * x)\n",
    "num_map_rdd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc14db26",
   "metadata": {},
   "source": [
    "# Take Action\n",
    "\n",
    "Now we'll ask to print the first 5 values from x^2. Using .take(N) will return an array with the first N elements. Spark will now compute the x^2 values since we asked for the values. This will be quicker since the RDD object is parallelized over 36 cores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a95c1132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2704, 8649, 225, 5184, 3721]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_map_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2f77fb",
   "metadata": {},
   "source": [
    "# Filter Transformation\n",
    "\n",
    "Next, let's use .filter() on the RDD object to return a new RDD object will only the numbers that pass the condition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da7c830a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values in new filter RDD:  28\n",
      "First 5 values in the new RDD:  [3, 2, 2, 3, 7]\n"
     ]
    }
   ],
   "source": [
    "num_filter_rdd = num_rdd.filter(lambda x: x < 10)\n",
    "print(\"Number of values in new filter RDD: \", str(num_filter_rdd.count()))\n",
    "print(\"First 5 values in the new RDD: \", str(num_filter_rdd.take(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ada401",
   "metadata": {},
   "source": [
    "# Collect Action\n",
    "\n",
    "Using .collect() on the RDD will return all the elements of the RDD object to a normal Python list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e25dc3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, [52, 93, 15, 72, 61])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_filter_array = num_rdd.collect()\n",
    "type(num_filter_array), num_filter_array[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab16841",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Working with Text Data\n",
    "\n",
    "In this example, we will use data from Project Gutenberg: \"The Hound of the Baskervilles, by Arthur Conan Doyle\". \n",
    "\n",
    "The download TXT file is at \"3070.txt\". \n",
    "\n",
    "We will load this TXT file to an RDD object with `.textFile()`. This will load the TXT by LINE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d246122b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Project Gutenberg's The Hound of the Baskervilles, by Arthur Conan Doyle\",\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with',\n",
       " 'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
       " 're-use it under the terms of the Project Gutenberg License included',\n",
       " 'with this eBook or online at www.gutenberg.org',\n",
       " '',\n",
       " '',\n",
       " 'Title: The Hound of the Baskervilles',\n",
       " '']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"3070.txt\"\n",
    "book_rdd = sc.textFile(path)\n",
    "\n",
    "# Print the first 10 lines of the Text\n",
    "book_rdd.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3920df3d",
   "metadata": {},
   "source": [
    "# Count Action\n",
    "\n",
    "We'll count the number of rows in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c4512c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7729"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7762d20d",
   "metadata": {},
   "source": [
    "# FlatMap Transformation\n",
    "\n",
    "Let's split the lines into words. We will split the book text into the individual words. The .flatMap() function can return multiple values for each element in the RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a15c682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 62248\n",
      "Number of distinct words: 9885\n"
     ]
    }
   ],
   "source": [
    "words_rdd = book_rdd.flatMap(lambda x: x.split())\n",
    "num_words = words_rdd.count()\n",
    "print(\"Number of words: \" + str(num_words))\n",
    "\n",
    "num_distinct_words = words_rdd.distinct().count()\n",
    "print(\"Number of distinct words: \" + str(num_distinct_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e9d030",
   "metadata": {},
   "source": [
    "# Map and ReduceByKey Transformations\n",
    "\n",
    "We'll create a new \"pair\" RDD `key_value_rdd` with key/value pairs. First, this RDD will have (\"word\",1) for each element of the RDD. Then we will use .reduceByKey() to combine all the same words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "009b2c6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(78, 'Project'), (217, 'The'), (11, 'Hound'), (1694, 'of'), (4, 'Arthur')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_value_rdd = words_rdd.map(lambda x: (x,1))\n",
    "word_kv_rdd = key_value_rdd.reduceByKey(lambda x,y: x+y)\n",
    "flip_word_kv_rdd = word_kv_rdd.map(lambda x: (x[1],x[0]))\n",
    "\n",
    "flip_word_kv_rdd.take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8120a92",
   "metadata": {},
   "source": [
    "# SortByKey Transformation\n",
    "\n",
    "We'll use .sortByKey() to sort the key/value pairs by decreasing occurrences of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f57ff549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3230, 'the'), (1694, 'of'), (1562, 'and'), (1449, 'to'), (1279, 'a')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_results_rdd = flip_word_kv_rdd.sortByKey(False)\n",
    "word_results_rdd.take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca4425",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "There are many other functions within Spark that can do more linguistics type tasks with an RDD object, like removing \"stop\" words and \"stemming\". This notebook provides a brief introduction to the basic operations you can perform with PySpark and RDDs.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mypyspark",
   "language": "python",
   "name": "mypyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
