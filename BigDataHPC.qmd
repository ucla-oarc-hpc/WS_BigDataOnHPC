---
title: "Big Data on HPC"
subtitle: "Introduction to Big Data with High-Performance Computing"
author: "Charles Peterson"
data: "June 29, 2023"
format: 
  revealjs: 
    transition: slide
    theme: simple
    scrollable: true
    self-contained: true
editor_options: 
  markdown: 
    wrap: 72
from: markdown+emoji
---

## :bulb: Overview

::: {style="font-size: 0.80em" }

In this workshop, we will dive into utilizing Big Data techniques with High-Performance Computing Resources :desktop_computer: :boom:

:::
:::: {.columns}
::: {.column width="40%"}
::: {style="font-size: 0.90em" .fragment}

We will cover:

- :mag: General concepts of Big Data

- :test_tube: Simple examples on Hoffman2

:::
:::
::: {.column width="40%"}
::: {style="font-size: 0.70em" .fragment}
::: {style="text-align: center"}

<img src="fullpic.png"/ width="50%">

:love_letter: Suggestions and Feedback, Email
[cpeterson\@oarc.ucla.edu](mailto:cpeterson@oarc.ucla.edu){.email}

:::
:::
:::
::::

## :file_folder: Files for this Presentation

::: {style="font-size: 0.80em" }

This presentation and accompanying materials are available on our UCLA OARC's GitHub repository :books:

:link: [UCLA OARC GitHub Repository](https://github.com/ucla-oarc-hpc/WS_BigDataOnHPC)

View slides in:

- :page_facing_up: PDF format - BigDataHPC.pdf

- :globe_with_meridians: HTML format: [Workshop Slides](https://ucla-oarc-hpc.github.io/WS_BigDataOnHPC)

You may want to install Spark/Dask now to follow along - `INTRO.md`

> **Note:** üõ†Ô∏è This presentation was built using [Quarto](https://quarto.org/) and RStudio.
    Quarto file: `BigDataHPC.qmd`

:::

## :thinking: What is Big Data?

The term **Big Data** refers to datasets, data processing, data modeling, and other data science tasks that become too large and complex for traditional techniques :boom:

:::{style="text-align: center"}
<img src="cats-bigdata.png" width="50%" alt="Big Data Cat"/>
:::

## :sparkles: Is Big Data for Me?

:::{.fragment style="text-align: center"}

Are you working with data? Then, absolutely YES! :tada:

Big Data provides solutions for diverse research areas, scaling up research to new heights! :rocket:

- Real-life Examples
  - Analyzing social media data for trends
  - Weather forecasting using data from sensors worldwide
  - Genomic data processing in biology

:::

## :construction: Challenges with LOTS of Data

::: {style="font-size: 0.90em"}
Projects with lots of **DATA** come with their own set of challenges üò∞:
:::
::: {style="font-size: 0.70em"}
::: {.fragment}
- :brain: Not enough RAM to accommodate large datasets.
:::

::: {.fragment}
- :hourglass_flowing_sand: Data processing is too time-consuming.
    - Traditional tasks struggle with large datasets.
    - They take forever to compute
:::

::: {.fragment}
- :robot: Machine Learning models grow complex.
    - Training sophisticated models may require computational resources for better accuracy.
:::

::: {.fragment}
High-Performance Computing (HPC) resources can supercharge üöÄ the solving of Big Data challenges by providing much more computing power than typical workstations :muscle:.

:::
:::

## The 3 V's of Big Data :globe_with_meridians:

Big Data is often characterized by:

::: {.fragment}
- **Volume**: The sheer size of data. :books:
:::

::: {.fragment}
- **Velocity**: The speed at which data is generated, processed, and transferred. :tornado:
    - Includes data transfer rates, in-memory processing vs. disk-based processing, etc.
:::

::: {.fragment}
- **Variety**: The diversity in types, sources, and nature of data. This includes structured vs unstructured data, and preprocessing or data cleanup requirements. :art:
:::

## The Other V's of Big Data :jigsaw:

::: {style="font-size: 0.90em"}

- **Value**: The potential insights and 'worth' that can be extracted from the data. :bulb:

- **Veracity**: The reliability, authenticity, and overall quality of the data. Includes handling missing values, data imputation, etc. :thumbsup:

- **Variability**: The adaptability of data in different formats, sources, and alignment with current data science methods. Raw and unstructured data can be tricky to manage. :arrows_clockwise:

üí° **Understanding your data can help you make informed decisions on which Big Data techniques to employ.**

:::

## :construction: Big Data Challenges

:::: {.columns}

:::{ .column style="text-align: center" }

<img src="dimensions_of_scale.svg" width="100%" />

:::
::: {.column style="font-size: 0.90em"}

- **Scaling Data Size** :chart_with_upwards_trend:
    - Datasets can become so large that they can't fit into RAM :scream:

- **Scaling Model/Task Size** :robot:
    - Machine Learning or other tasks become so complex that a single CPU core is not adequate :snail:

:::
::::

::: footer
Image source - DASK <https://ml.dask.org/index.html>
:::

## :blue_book: Big Data and HPC Concepts

::: {style="font-size: 0.65em"}

- **HPC**
  - High-Performance Computing involves the use of powerful processors, networks, and parallel processing techniques
  - **Cluster**: A group of computers working together
  - **Node**: A single computer in a cluster
  
- **Parallel Computing** :vertical_traffic_light:
    - Executing tasks simultaneously over multiple CPUs can make code run faster :racing_car:
    - Typical Python/R tasks may run on a single CPU core.
    - Many packages/libraries are available for running tasks over multiple CPUs:
        - Python: [Python Parallel Processing](https://wiki.python.org/moin/ParallelProcessing)
        - R: [R Parallel Computation](https://bookdown.org/rdpeng/rprogdatascience/parallel-computation.html)

::: {.fragment}

- **Cluster Computing** :desktop_computer: :link:
    - Involves multiple computing resources working together.
    - HPC resources like Hoffman2 are excellent examples:
        - Multiple compute nodes with access to many cores.

:::
:::

## :monocle_face: More Concepts

::: { style="font-size: 0.80em"}

- **Symmetric Multiprocessing (SMP) Programming** :brain: :briefcase:
    - SMP sets up parallel tasks over shared memory.
    - Typically, these tasks can ONLY run on a single compute node.
    - Uses a single process over multiple threads.

::: {.fragment}

- **Cluster or Distributed Programming** :globe_with_meridians: :computer:
    - Distributes tasks over multiple processes.
    - Allows tasks to be distributed across multiple compute nodes.

:::
:::

## :gear: Even More Concepts

::: { style="font-size: 0.80em"}

- **Lazy Evaluation** :hourglass_flowing_sand:
    - Delays a task or expression until the resulting value is needed.
    - Functions and calculations can be set up so they can later run in parallel.

::: {.fragment}

- **In-memory Computing** :rocket:
    - Performs tasks on data in the RAM (memory) of the computer.
    - Data is computed in RAM instead of disk for faster processing.

:::
:::

## :hammer_and_wrench: Big Data Tools

Various frameworks, APIs, and libraries for Big Data projects:

![](sparklogo.png){.absolute top=200 left=0 width="350" height="200"}

![](dasklogo.jpeg){.absolute top=150 right=50 width="450" height="250"}

![](h2ologo.jpg){.absolute bottom=20 right=400 width="300" height="200"}
![](vaexlogo.png){.absolute bottom=0 right=20 width="300" height="100"}
![](rapidslogo.png){.absolute bottom=0 left=20 width="300" height="300"}
![](hadooplogo.png){.absolute bottom=150 right=20 width="300" height="100"}


# Spark :zap:

![](sparklogo.png)

## Components of Spark :gear:

![](sparkcore.png)

::: { style="font-size: 0.60em"}

- Spark has APIs that can work with 
    - Python (PySpark) :snake: 
    - R (SparkR) 

:::

## Spark Data :floppy_disk:

RDD - Resilient Distributed Dataset

<img src="RDD.png" />

- Large datasets can be distributed across multiple compute nodes :computer:

## Data Persistence :cd:

Spark supports different levels of persistence for performance optimization.

::: { style="font-size: 0.65em"}

- MEMORY_ONLY
    - All RDD data is stored in-memory :brain:
      - Fastest processing time, but requires RAM to fit all data
    
- MEMORY_AND_DISK
    - RDD data is stored in memory and spills to disk if necessary
    - Balances between memory usage and processing speed
    
- DISK_ONLY
    - RDD data is stored only on disk
    - Minimal memory usage, but slower processing time

:::

## Spark DataFrames :page_with_curl:

Along with RDDs, Spark also has an API for DataFrames (similar to Pandas).

</br>

- SQL-like library that can handle data with named columns
- Great for structured/semi-structured data :bar_chart:

## Spark Session :door:

::: { style="font-size: 0.80em"}

SparkSession is the entry point for using the DataFrame and Dataset API.

- Underlying Spark functionality
- Creates the main Spark driver
- Used to create DataFrames and SQL-like tasks

```{.python}
spark = SparkSession.builder \
          .appName("MyPySpark") \
          .config("spark.driver.memory", "15g") \
          .getOrCreate()
```


SparkContext is the entry point for creating RDDs.

- Created with the `spark` object from `SparkSession.builder`

```{.python}
sc = spark.sparkContext
```

:::

## Installing PySpark :wrench:

::: {style="font-size: 0.80em"}

Easiest way to install PySpark is by anaconda3. 

This is great when running PySpark on a single compute node.

```{.bash}
module load anaconda3
conda create -n mypyspark openjdk pyspark python=3.9 \
                          pyspark=3.3.0 py4j jupyterlab findspark \
                          h5py pytables pandas \
                          -c conda-forge -y
conda activate mypyspark
pip install ipykernel
ipython kernel install --user --name=mypyspark
```

::: {.fragment}

This will create a conda env named, mypyspark, with access to Jupyter

This conda env will have both Spark and PySpark installed

:::{.callout-note}

Information on using Anaconda can be found from a previous workshop

<https://github.com/ucla-oarc-hpc/H2HH_anaconda>

:::
:::
:::

## PySpark: Basic Operations :clipboard:

::: {style="font-size: 0.80em"}

Let's practice basic PySpark functions with examples.

- Download the workshop content from the GitHub repository
- We'll work with a Jupyter Notebook: Spark_basics.ipynb
- Jupyter Notebook: `Spark_basics.ipynb` from `spark-ex1`

```{.bash}
cd $SCRATCH
git clone https://github.com/ucla-oarc-hpc/WS_BigDataOnHPC
cd WS_BigDataOnHPC
cd spark-ex1
```

::: {.fragment}

We will download "The Hound of the Baskervilles", by Arthur Conan Doyle

- Data source: [Project Gutenberg](https://www.gutenberg.org/)

```{.bash}
wget https://www.gutenberg.org/files/3070/3070.txt
```

:::
:::

## PySpark: Basic operations: Starting the notebook

::: {style="font-size: 0.65em"}

We will use the `h2jupynb` script to start Jupyter on Hoffman2

You will run this on your LOCAL computer.

```{.bash}
wget https://raw.githubusercontent.com/rdauria/jupyter-notebook/main/h2jupynb
chmod +x h2jupynb

#Replace 'joebruin' with you user name for Hoffman2
#You may need to enter your Hoffman2 password twice 

python3 ./h2jupynb -u joebruin -t 5 -m 10 -e 2 -s 1 -a intel-gold\\* \
                    -x yes -d /SCRATCH/PATH/WS_BigDataOnHPC/spark-ex1
```

:::{.callout-note}

The `-d` option in the `python3 ./h2jupynb` will need to have the `$SCRATCH/WS_BigDataOnHPC` full PATH directory
:::

This will start a Jupyter session on Hoffman2 with ONE entire intel-gold compute node (36 cores)

More information on the `h2jupynb` can be found on the [Hoffman2 website](<https://www.hoffman2.idre.ucla.edu/Using-H2/Connecting/Connecting.html#connecting-via-jupyter-notebook-lab)

:::

## PySpark: Machine Learning :robot:

::: {style="font-size: 0.85em"}

This example will use Spark's Machine Learning library (MLlib)

We will use data from the [Million song subset](http://millionsongdataset.com/)

This subset has ~500,000 songs with:

- Year of the song
- 90 features relating to the timbre average and covariance of the song 

::: {.fragment}

Download the dataset

```{.bash}
cd $SCRATCH/WS_BigDataOnHPC
cd spark-ex2
wget https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip
unzip YearPredictionMSD.txt.zip
```

:::

::: {.fragment}

We will use multiple nodes to run Spark

:::
:::

## PySpark: Multi-node setup

::: {style="font-size: 0.70em"}

In the previous example, we used pyspark with 1 (36-core) compute node. 

- We will run PySpark over multiple nodes. This will:
    - increase the number of cores  
    - increase the available RAM to fit a large dataset

To do this, we will NOT use the Spark installation from our conda install, but use spark from a build that we will download from the spark website.

```{.bash}
mkdir -pv $SCRATCH/WS_BigDataOnHPC/apps/spark
cd $SCRATCH/WS_BigDataOnHPC/apps/spark

wget https://archive.apache.org/dist/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz
tar -vxf spark-3.3.0-bin-hadoop3.tgz
```

:::{.callout-note}

Though we will not use the Spark from conda, we will still use the PySpark package that was install with conda. The Spark and PySpark packages will need to be the same version (3.3.0 in this example)

:::
:::


## PySpark: Multi-node setup - Starting the notebook :satellite:

::: {style="font-size: 0.75em"}

Since we are using our Spark build that we just downloaded, we will start spark and submit it as a job, then connect to jupyter.

- Example: `spark-ex2`
    - Job script `pyspark-multi-jupyter.job`  
    - Starts Spark and a Jupyter session for us to connect.

In this example, we will use 3 compute nodes in total.

- One compute node will be the master that will run the Spark driver.
- The other two will be workers that will run the tasks

:::{.callout-tip}

For large data jobs, I like to have the Spark driver to be separate from the workers.

Large data jobs may require the Spark driver to have a heavy CPU load and memory.

:::
:::

## PySpark: Multi-node setup - Starting the notebook :notebook_with_decorative_cover:

::: {style="font-size: 0.70em"}

- Submitting the Job

```{.bash}
qsub pyspark-multi-jupyter.job
```

- Once the job starts, we will connect to this Jupyter session
    - The `spark-test.JOBID` file will display the MASTER node name
    - This master node will have the Spark driver and Jupyter process

Run this `ssh -L` command on your LOCAL computer

```{.bash}
# Replace NODENAME with the name of the MASTER node
# Replace joebruin with you Hoffman2 user name
ssh -L 8888:NODENAME:8888 joebruin@hoffman2.idre.ucla.edu
```

- This will create a SSH tunnel to the master compute node so we can open Jupyter at `http://localhost:8888`

- Then we can open the notebook named `MSD.ipynb`

:::

## Spark dashboard :chart_with_upwards_trend:

::: {style="font-size: 0.85em"}

Spark has a visual dashboard that can view the tasks in real-time

- By default, Spark will run this dashboard on port 4040

- Create a ssh tunnel to the compute node to view the dashboard on your local machine

</br>

```{.bash}
ssh -L 4040:NODENAME:4040 joebruin@hoffman2.idre.ucla.edu
```

You will need to replace NODENAME with the master compute node that has your Spark job

:::

## Spark batch job :hourglass:

You can run Spark as a batch job to run non-interactively.

- We will use the command `spark-submit` to start the pyspark calucation located at:
    - `$SPARK_HOME/bin/spark-submit`

</br>

```{.bash}
qsub pyspark-multi-batch.job
```

## Spark Bonus Example :gift:

I have another Machine Learning example for Spark that I may not have time to go over in this workshop.

In this example, we will train a Machine Learning model using data from [LIBSVM](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/)

- Inside of `spark-bonus`
    - Jupyter notebook: `ML-bonus.ipynb`
    
- Download the dataset

```{.bash}
cd $SCRATCH/WS_BigDataOnHPC
cd spark-ex2
wget https://raw.githubusercontent.com/apache/spark/master/data/mllib/sample_libsvm_data.txt
```

# Dask :hammer_and_wrench:

![](dasklogo.jpeg)

## Intro to Dask :bulb:

- Dask is a parallel computing library for Python :snake:
- Dask uses multiple cores to run tasks :computer:
- Can use GPUs to speed up tasks :rocket:
- Very minimal changes to Python code is required for Dask
  - Dask Arrays and Dataframes are similar to Numpy and Pandas

## Dask Array :bar_chart:

::: { style="font-size: 0.75em"}

Dask has an Arrays API created from NumPy-like chunks

:::

:::: {.columns}

::: { .column width="40%"}

::: {style="text-align: center" .fragment}

::: { style="font-size: 0.80em"}

</br>

</br>


Typically Numpy code :scroll:

```{.python}
import numpy as np

numpy_array = np.ones((100))

print(np.average(numpy_array))

```
 
:::
 
:::

:::

::: { .column width="60%" }

::: {style="text-align: center" .fragment}

::: { style="font-size: 0.80em"}

![](daskarray.svg)

Dask code :scroll:

```{.python}
import dask.array as da

dask_array = np.ones((100), chunks=(10))

print(da.average(dask_array).compute())
```

:::

:::

:::

::::

::: {.fragment style="font-size: 0.60em"}

- Dask Arrays can process chunks over multiple cores :brain:
- Great for larger-than-memory arrays; chunks are computed in memory
- Dask Arrays have similar functions and methods as Numpy objects

:::


::: {.footer}

Image source - https://docs.dask.org/en/stable/

:::

## Dask DataFrames :page_facing_up

Dask DataFrames are Pandas-like objects and are composed of Pandas-like "chunks".

:::: {.columns}

::: { .column width="40%" }

![](daskdataframe.svg)
:::

::: { .column width="60%" }

</br>

- Can lazily read data files (CSV, hdf5, etc.)


:::

::::

::: {.footer}

Image source - https://docs.dask.org/en/stable/

:::


## Dask installation :wrench:

::: { style="font-size: 0.90m"}

```{.bash}
module load anaconda3
conda create -n mydask python pandas jupyterlab  joblib seaborn \
                       dask dask-ml nodejs graphviz python-graphviz \
                       -c conda-forge -y
conda activate mydask
pip install ipykernel
ipython kernel install --user --name=mydask
```

This will create a conda env, `mydask`, that will have

- Dask
- Dask-ml (Machine Learning)
- Jupyter
- scikit-learn/joblib

:::

## Dask: Basic Operations :clipboard:

::: {style="font-size: 0.75em"}

We will use the `h2jupynb` script to start Jupyter on Hoffman2

You will run this on your **LOCAL** computer.

```{.bash}
python3 ./h2jupynb -u joebruin -t 5 -m 10 -e 2 -s 1 -a intel-gold\\* -x yes \
                   -d /SCRATCH/PATH/WS_BigDataOnHPC/dask-ex1
```

Replace `joebruin` with your Hoffman2 user account.

Replace `/SCRATCH/PATH/WS_BigDataOnHPC` with the full PATH name of the workshop on Hoffman2

- Let's go to `dask-ex1`
    - Jupyter notebook `dask_basic.ipynb` 

:::

## Dask: Machine Learning :robot:

Dask has a Dask-ML library with scalable Machine Learning methods. There is also integration with:

- Scikit-Learn and Joblib
- XGBoost
- PyTorch
- TensorFlow and Keras

## Dask ML Example :chart_with_upwards_trend:

::: {style="font-size: 0.75em"}

This example will use Scikit-Learn with Dask

We will use data from the Million song subset

- Download the dataset

```{.bash}
cd $SCRATCH/WS_BigDataOnHPC/dask-ex2
wget https://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip
unzip YearPredictionMSD.txt.zip
```

- Start Jupyter notebook
    - Replace `joebruin` with your Hoffman2 user account
    - Replace `/SCRATCH/PATH/WS_BigDataOnHPC` with the full PATH name of the workshop on Hoffman2

```{.bash}
python3 ./h2jupynb -u joebruin -t 5 -m 10 -e 2 -s 1 -a intel-gold\\* -x yes -d /SCRATCH/PATH/WS_BigDataOnHPC/dask-ex2
```

- Jupyter Notebook: `MSD-dask.ipynb`

:::

## Dask dashboard :chart_with_upwards_trend:

::: {style="font-size: 0.80em"}

Dask has a visual dashboard that can view the tasks in real-time

- By default, Dask will run this dashboard on port 8787

</br>

- Create a ssh tunnel to the compute node to view the dashboard on your local machine

```{.bash}
ssh -L 8787:NODENAME:8787 joebruin@hoffman2.idre.ucla.edu
```

You will need to replace NODENAME with the compute node that has your Dask job

:::


# Wrap-up :star:

## Big Data on HPC: A Taste of Possibilities

::: {style="font-size: 0.65em"}

- :rocket: Just the Beginning: Today‚Äôs experience is a glimpse into the vast world of Big Data.
- :earth_americas: Big Data for All: Accessible to every field! :busts_in_silhouette:
- :bulb: Scale Up Your Research: Enhance your analysis capabilities regardless of your study or profession.
  - :mag: Analyze More, Larger Datasets: Delve deeper into your data.
  - :gear: Complex Methods & Models: Execute more sophisticated techniques.
- :brain: Think Critically about your data processing:
  - :file_folder: Data Storage: Reflect on how your data is stored.
  - :computer: Parallel Processing: Consider the number of CPU cores running in parallel.
  - :abacus: Memory Matters: Be mindful of memory considerations.
- :hammer_and_wrench: Powerful Frameworks:
  - :sparkles: Spark & Dask: Just two of the many robust Big Data processing frameworks available.

:::

## Final thoughts :thought_balloon:

::: {style="font-size: 0.80em"}

- :gear: CPU/Cores Utilization: Keep track of CPU and core usage:
  - :clipboard: Did you request the same amount of cores via SGE?
  - :wrench: Is your code set up to run with multiple cores?
  - :computer: Multi-node Computing: Are you using multiple compute nodes?
- :brain: Memory Requirements: Evaluate how much memory is needed.
  - :floppy_disk: Data in Memory: Does all your data fit in memory?
  - :rocket: Out-of-Memory Tasks: Are you utilizing tasks that exceed available memory?
  
Optimize your resources for seamless project execution! :muscle::chart_with_upwards_trend:


:::

## Thank you! :heart:

::: { style="font-size: 0.70em" }

Questions? Comments?

- [cpeterson\@oarc.ucla.edu](mailto:cpeterson@oarc.ucla.edu){.email}

- Look at for more Hoffman2 workshops at [https://idre.ucla.edu/calendar ](https://idre.ucla.edu/calendar)

:::{ style="text-align: center" }

<img src="padfoot.jpeg"/ width="50%" height="50%">

:::
:::